{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments, AdamW\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BTC dataset\n",
    "btc_dataset = load_dataset('tner/btc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = btc_dataset['train']\n",
    "test_dataset = btc_dataset['test']\n",
    "\n",
    "# Display example tweets and labels\n",
    "print(\"Example tweets: \\n\")\n",
    "for example in train_dataset['tokens'][:3]:\n",
    "    print(example)\n",
    "\n",
    "# Display example tweets and labels\n",
    "print(\"Example labels: \\n\")\n",
    "for label in train_dataset['tags'][:3]:\n",
    "    print(label)\n",
    "\n",
    "# Check train/test splits\n",
    "train_size = len(train_dataset)\n",
    "test_size = len(test_dataset)\n",
    "print(f\"\\nTrain Size: {train_size}\")\n",
    "print(f\"Test Size: {test_size}\")\n",
    "\n",
    "# Visualize the distribution of labels\n",
    "labels = test_dataset['tags']\n",
    "plt.hist(labels, bins=round(len(labels)/100), edgecolor='black')\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split 'train' further to create a 'validation' set, directly using DatasetDict for clarity\n",
    "\n",
    "split_datasets = btc_dataset['train'].train_test_split(train_size=0.7, seed=42)\n",
    "btc_with_validation_dataset = DatasetDict({\n",
    "    'train': split_datasets['train'],\n",
    "    'validation': split_datasets['test'],  # Rename test split as the validation dataset\n",
    "    'test': btc_dataset['test']  # Include original test set\n",
    "})\n",
    "btc_with_validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to sub-sample each dataset split\n",
    "def sub_sample_dataset(dataset_dict, percentage=0.1, seed=42):\n",
    "    random.seed(seed)  # Ensures reproducibility\n",
    "    subsets = {}\n",
    "   \n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        size = int(dataset_dict[split].num_rows * percentage)\n",
    "        indices = random.sample(range(dataset_dict[split].num_rows), size)\n",
    "        subsets[split] = dataset_dict[split].select(indices)\n",
    "   \n",
    "    return DatasetDict(subsets)\n",
    "\n",
    "# Use the function to sub-sample the dataset\n",
    "subset_dataset_dict = sub_sample_dataset(btc_with_validation_dataset)\n",
    "subset_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odel_checkpoint = \"distilbert/distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Check if the tokenizer is fast\n",
    "print(tokenizer.is_fast)\n",
    "\n",
    "# Print tokens and word IDs of an example of the training set\n",
    "inputs = tokenizer(subset_dataset_dict[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "print(inputs.tokens(), '\\n', inputs.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
